{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align=\"center\" dir=\"auto\">\n",
        "<p dir=\"auto\">\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/write-with-neurl/modelbit-notebooks/blob/main/deploy-tinyvicuna/Deploy_Tiny_Vicuna_1B_With_ModelBit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz5WHWrn6W1b"
      },
      "source": [
        "# ⚡ Deploying Tiny Vicuna 1B LLM to A Rest API Endpoint for Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFdkseSm-bRY"
      },
      "source": [
        "In this example, we'll use hugging face to deploy a Tiny Vicuna 1B model as a REST endpoint for text generation inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDyFaZY16Ol_"
      },
      "source": [
        "## 🧑‍💻 Installations and Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RggB4Ix3bXn",
        "outputId": "bea85e2a-2df4-43d4-e82c-9575abd6552e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (3.20.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.19.4)\n",
            "Collecting modelbit\n",
            "  Downloading modelbit-0.31.6-py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.0/114.0 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Collecting pycryptodomex (from modelbit)\n",
            "  Downloading pycryptodomex-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from modelbit) (1.5.3)\n",
            "Collecting types-requests (from modelbit)\n",
            "  Downloading types_requests-2.31.0.10-py3-none-any.whl (14 kB)\n",
            "Collecting types-PyYAML (from modelbit)\n",
            "  Downloading types_PyYAML-6.0.12.12-py3-none-any.whl (14 kB)\n",
            "Collecting types-pkg-resources (from modelbit)\n",
            "  Downloading types_pkg_resources-0.1.3-py2.py3-none-any.whl (4.8 kB)\n",
            "Collecting zstandard (from modelbit)\n",
            "  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from modelbit) (1.4.4)\n",
            "Collecting texttable (from modelbit)\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: build in /usr/local/lib/python3.10/dist-packages (from modelbit) (1.0.3)\n",
            "Collecting pkginfo (from modelbit)\n",
            "  Downloading pkginfo-1.9.6-py3-none-any.whl (30 kB)\n",
            "Collecting boto3>=1.23.0 (from modelbit)\n",
            "  Downloading boto3-1.34.7-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<2,>=1.21.1 (from modelbit)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting build (from modelbit)\n",
            "  Downloading build-0.10.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from modelbit) (7.0.0)\n",
            "Collecting botocore<1.35.0,>=1.34.7 (from boto3>=1.23.0->modelbit)\n",
            "  Downloading botocore-1.34.7-py3-none-any.whl (11.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.23.0->modelbit)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.23.0->modelbit)\n",
            "  Downloading s3transfer-0.10.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build->modelbit) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build->modelbit) (2.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->modelbit) (3.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->modelbit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->modelbit) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "INFO: pip is looking at multiple versions of types-requests to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting types-requests (from modelbit)\n",
            "  Downloading types_requests-2.31.0.9-py3-none-any.whl (14 kB)\n",
            "  Downloading types_requests-2.31.0.8-py3-none-any.whl (14 kB)\n",
            "  Downloading types_requests-2.31.0.7-py3-none-any.whl (14 kB)\n",
            "  Downloading types_requests-2.31.0.6-py3-none-any.whl (14 kB)\n",
            "Collecting types-urllib3 (from types-requests->modelbit)\n",
            "  Downloading types_urllib3-1.26.25.14-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->modelbit) (1.16.0)\n",
            "Installing collected packages: types-urllib3, types-PyYAML, types-pkg-resources, texttable, sentencepiece, zstandard, urllib3, types-requests, pycryptodomex, pkginfo, jmespath, build, botocore, s3transfer, boto3, modelbit\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: build\n",
            "    Found existing installation: build 1.0.3\n",
            "    Uninstalling build-1.0.3:\n",
            "      Successfully uninstalled build-1.0.3\n",
            "Successfully installed boto3-1.34.7 botocore-1.34.7 build-0.10.0 jmespath-1.0.1 modelbit-0.31.6 pkginfo-1.9.6 pycryptodomex-3.19.0 s3transfer-0.10.0 sentencepiece-0.1.99 texttable-1.7.0 types-PyYAML-6.0.12.12 types-pkg-resources-0.1.3 types-requests-2.31.0.6 types-urllib3-1.26.25.14 urllib3-1.26.18 zstandard-0.22.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers sentencepiece torch protobuf huggingface_hub modelbit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1Geel9V1-4g"
      },
      "outputs": [],
      "source": [
        "import modelbit\n",
        "import requests\n",
        "from huggingface_hub import snapshot_download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmMm0hDw8ted"
      },
      "source": [
        "## Loading and Caching of Tiny Vicuna 1B and Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVfh1KN07HgA"
      },
      "source": [
        "We'll be setting up a function to load and cache a language model and its tokenizer for efficient usage. The first part involves importing necessary modules: `AutoModelForCausalLM` and `AutoTokenizer` from the `transformers` library, and `cache` from `functools`.\n",
        "\n",
        "The `get_vicuna_model` function, decorated with `@cache`, is our key player. This function uses `snapshot_download` to fetch a specific model (here, https://huggingface.co/Jiayi-Pan/Tiny-Vicuna-1B/tree/main \"Tiny-Vicuna-1B\") and initializes both the tokenizer and the model using the `AutoTokenizer.from_pretrained` and `AutoModelForCausalLM.from_pretrained` methods, respectively.\n",
        "\n",
        "The use of `@cache` is a clever optimization; it ensures that once the model and tokenizer are loaded, they are stored in memory. This significantly speeds up future calls to this function, as it avoids reloading the model and tokenizer from scratch each time, making it ideal for deployments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhk__dD0CqeT"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from functools import cache\n",
        "\n",
        "@cache\n",
        "def get_vicuna_model():\n",
        "    model_path = snapshot_download(repo_id=\"Jiayi-Pan/Tiny-Vicuna-1B\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnUxC0dz9LCA"
      },
      "source": [
        "## Inference Function for Generating Responses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yny0tOdV9trY"
      },
      "source": [
        "In this section, we write a function for inference, `tiny_vicuna_inference` function. Upon receiving a text prompt as input, the first step within the function is to retrieve the pre-loaded Vicuna model and tokenizer by calling `get_vicuna_model()`. This efficient retrieval is thanks to our previously established caching mechanism.\n",
        "\n",
        "Next, the function encodes the input prompt using the tokenizer, preparing it in a format suitable for the model, and specifies that the output should be PyTorch tensors (`return_tensors=\"pt\"`). The model then steps in to generate a response based on these inputs, with an upper limit of 512 tokens for the response length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sLK4yRF8gmE"
      },
      "outputs": [],
      "source": [
        "def tiny_vicuna_inference(prompt: str) -> str:\n",
        "    model, tokenizer = get_vicuna_model()\n",
        "\n",
        "    # Encode the prompt and generate a response\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs, max_length=512)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLUv1MIDAZ8E"
      },
      "source": [
        "## 🚀 Deploying Model to a REST API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NDMGwyy9788"
      },
      "source": [
        "### Checking and Displaying Versions of Key Python Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QClLneKs-7y6"
      },
      "source": [
        "In preparation for deployment using [Modelbit](https://doc.modelbit.com/getting-started/), where specific library versions are a requirement, this script checks and displays the versions of key Python libraries. It uses `pkg_resources` to fetch version information for `transformers`, `sentencepiece`, and `torch`, which are essential for running inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXanBRG1Cvg_",
        "outputId": "8db2ea23-a9ac-401d-cdca-6a2069445330"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Transformers version: 4.35.2\n",
            "🔍 Sentencepiece version: 0.1.99\n",
            "🔍 Torch version: 2.1.0+cu121\n"
          ]
        }
      ],
      "source": [
        "import pkg_resources\n",
        "\n",
        "# Check versions of transformers, sentencepiece and torch\n",
        "transformers_version = pkg_resources.get_distribution(\"transformers\").version\n",
        "sentencepiece_version = pkg_resources.get_distribution(\"sentencepiece\").version\n",
        "torch_version = pkg_resources.get_distribution(\"torch\").version\n",
        "\n",
        "print(f\"🔍 Transformers version: {transformers_version}\")\n",
        "print(f\"🔍 Sentencepiece version: {sentencepiece_version}\")\n",
        "print(f\"🔍 Torch version: {torch_version}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOjIn-lC_TSl"
      },
      "source": [
        "### 🔐 Log into `modelbit`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAhnD8cB_Mib"
      },
      "outputs": [],
      "source": [
        "# Log into Modelbit\n",
        "mb = modelbit.login(branch=\"main\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afT2SBsy_tA6"
      },
      "source": [
        "We are now ready to deploy our model to a REST API Endpoint on Modelbit. For this deployment, we'll use the `tiny_vicuna_inference` function, which encapsulates the entire process of loading the model and performing inference. This function simply takes a text prompt as input and efficiently generates the corresponding text output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "28482Dr3CzQ2",
        "outputId": "0794cc57-d93d-4e75-8988-b7cf67333b2d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"margin: 0; padding: 5px; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">\n",
              "  <div>\n",
              "    <span style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; font-weight: bold; color: #15803d;\">Deploying </span> <span style=\"margin: 0; padding: 3px; line-height: 1.75; font-size: 13px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: 400; background-color: rgba(209, 213, 219, 0.2);\">tiny_vicuna_inference</span>\n",
              "  </div>\n",
              "  \n",
              "  \n",
              "\n",
              "\n",
              "  <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; margin-top: 10px;\">\n",
              "    <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; font-weight: bold; color: #845B99;\">Heads up!</div>\n",
              "    <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\" id=\"fst-mb-484583605\">\n",
              "      <div style=\"margin: 0; padding: 5px; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; border-left: 1px solid #845B99; margin-bottom: 10px;\">\n",
              "\n",
              "    \n",
              "      <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">\n",
              "        You chose <span style=\"margin: 0; padding: 3px; line-height: 1.75; font-size: 13px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: 400; background-color: rgba(209, 213, 219, 0.2);\">transformers==4.36.2</span> for your production environment,\n",
              "        but you have <span style=\"margin: 0; padding: 3px; line-height: 1.75; font-size: 13px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: 400; background-color: rgba(209, 213, 219, 0.2);\">transformers==4.35.2</span> locally.\n",
              "      </div>\n",
              "\n",
              "      <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">To match your environment to production, run:</div>\n",
              "      <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; padding-left: 15px;\">\n",
              "        <span style=\"margin: 0; padding: 3px; line-height: 1.75; font-size: 13px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: 400; background-color: rgba(209, 213, 219, 0.2);\">!pip install transformers==4.36.2</span>\n",
              "      </div>\n",
              "\n",
              "\n",
              "      <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">To match production to your local environment, include this line in your deployment:</div>\n",
              "      <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; padding-left: 15px;\">\n",
              "        <span style=\"margin: 0; padding: 3px; line-height: 1.75; font-size: 13px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: 400; background-color: rgba(209, 213, 219, 0.2);\">mb.deploy(my_deploy_function, <b>python_packages=[\"transformers==4.35.2\"]</b>)</span>\n",
              "      </div>\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "  </div>\n",
              "    </div>\n",
              "    \n",
              "  </div>\n",
              "\n",
              "  \n",
              "\n",
              "\n",
              "  \n",
              "\n",
              "\n",
              "\n",
              "\n",
              "  <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">Uploading dependencies...</div>\n",
              "</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div style=\"margin: 0; padding: 5px; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">\n",
              "  <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; font-weight: bold; color: #15803d;\">Success!</div>\n",
              "  \n",
              "    <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">\n",
              "      Deployment <span style=\"margin: 0; padding: 3px; line-height: 1.75; font-size: 13px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: 400; background-color: rgba(209, 213, 219, 0.2);\">tiny_vicuna_inference</span>\n",
              "      will be ready in  a few seconds!\n",
              "    </div>\n",
              "  \n",
              "\n",
              "  <a href=\"https://us-east-1.modelbit.com/w/silasbempong/main/deployments/tiny_vicuna_inference/apis\" target=\"_blank\" style=\"display: inline-block; margin-top: 12px;\" >\n",
              "    <div\n",
              "      style=\"display: inline-block; background-color: #845B99; border-radius: 0.375rem; color: white; cursor: pointer; font-size: 14px; font-weight: 700; padding: 8px 16px;\"\n",
              "      onmouseenter=\"this.style.background='#714488'\"\n",
              "      onmouseleave=\"this.style.background='#845B99'\"\n",
              "    >\n",
              "      View in Modelbit\n",
              "    </div>\n",
              "  </a>\n",
              "</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Deploy the inference function to ModelBit\n",
        "mb.deploy(tiny_vicuna_inference, python_packages=[\"transformers==4.36.2\", \"sentencepiece==0.1.99\", \"torch==2.1.2\"], require_gpu=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgTbOlS6AoDT"
      },
      "source": [
        "## 📩 Test the REST Endpoint with a Prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oD61lUTCCTE"
      },
      "source": [
        "In this section, we test the deployed `tiny_vicuna_inference` model using a Python function. The function `test_vicuna_inference` makes a POST request to the Modelbit endpoint, sending a text prompt and receiving the generated text in return."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nfYBhTBFY9a"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "def test_vicuna_inference(prompt: str):\n",
        "    # Construct the URL for the ModelBit endpoint\n",
        "    url = \"https://ENTER_WORKSPACE_NAME.us-east-1.modelbit.com/v1/tiny_vicuna_inference/latest\"\n",
        "    # Set the headers to indicate JSON content type\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    # Format the data payload as JSON, with 'prompt' as a key\n",
        "    data = json.dumps({\"data\": prompt})\n",
        "    # Make the POST request and return the JSON response\n",
        "    response = requests.post(url, headers=headers, data=data)\n",
        "    return response.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QMhNaPNFcFH",
        "outputId": "636f42a9-9466-448e-ee78-8b3e86e43ed4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'data': 'My name is Clara and I am a student at the University of California, Berkeley. I am currently pursuing a degree in Computer Science. I am interested in learning about the field of computer science and how it can be applied to real-world problems. In my free time, I enjoy playing video games, reading, and exploring new places.'}\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "test_prompt = \"My name is Clara and I am\"\n",
        "print(test_vicuna_inference(test_prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cJ2-bFUAzof"
      },
      "source": [
        "You can also test your endpoint from the command line using:\n",
        "\n",
        "\n",
        "> `curl -s -XPOST \"https://ENTER_WORKSPACE_NAME.us-east-1.modelbit.com/v1/tiny_vicuna_inference/latest\" -d '{\"data\": \"Once upon a time,\"}' | json_pp`\n",
        "\n",
        "---\n",
        "> ⚠️ Replace the `ENTER_WORKSPACE_NAME` placeholder with your workspace name."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🚀 Model Hub by Modelbit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interested in more notebooks like this to deploy LLMs? Check out Model Hub by Modelbit ⚡️: https://www.modelbit.com/model-hub"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
